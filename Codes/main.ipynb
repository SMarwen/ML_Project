{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal # Signal Processing Library\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd # DataBase management\n",
    "import seaborn # Improve Images\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Column Names ##\n",
    "nm = ['ind', 'ax', 'ay', 'az', 'label'] # Names of columns\n",
    "acc_nm = ['ax', 'ay', 'az'] # Names of signal features\n",
    "## Load Dataset\n",
    "link = \"Dataset/1.csv\"\n",
    "def loadDB(link):\n",
    "    \"A function that import the database from Dataset folder and return df\"\n",
    "    \n",
    "    label2str = {1:'Working at Computer', 2:'Standing Up, Walking and Going up-down stairs', \n",
    "                 3:'Standing', 4:'Walking',5:'Going Up\\Down Stairs', 6:'Walking and Talking with Someone', \n",
    "                 7:'Talking while Standing'}\n",
    "    df = pd.read_csv(link, sep=',', names=nm)\n",
    "    del df['ind']\n",
    "    df = df[df.label != 0] # Unusable row\n",
    "    df['label_str'] = df.label.apply(lambda x:label2str[x]) # Important to some plots\n",
    "    return df\n",
    "\n",
    "df_raw = loadDB(link)\n",
    "\n",
    "def widen_signal(df):\n",
    "    # Magnitude\n",
    "    df['mag'] = np.sqrt(np.square(df[acc_nm]).sum(axis=1)) \n",
    "    # Median filter - 3rd ordre\n",
    "    def med_fil(df, names):\n",
    "        \"\"\"Filter the signal by a median filter\"\"\"\n",
    "        df_r = pd.DataFrame()\n",
    "        df = df[names]\n",
    "        for column in df.columns:\n",
    "            name = column+'_median'\n",
    "            df_r[name] = signal.medfilt(df[column].values)\n",
    "        return df_r\n",
    "    df_med = med_fil(df, acc_nm)\n",
    "    # Diffrential\n",
    "    def diffrential(df, names):\n",
    "        \"\"\"Compute the differentials of acceleration - Jerk\"\"\"\n",
    "        df = df[names]\n",
    "        df_r = df.diff(periods=1, axis=0).fillna(method='backfill')\n",
    "        df_r.columns = [names[0]+'_diff', names[1]+'_diff', names[2]+'_diff']\n",
    "        return df_r\n",
    "    df_diff = diffrential(df, acc_nm)\n",
    "    # Low pass filter\n",
    "    def lowpass(df, names):\n",
    "        \"\"\"Compute low-pass filter\"\"\"\n",
    "        df = df[names]\n",
    "        df_r = pd.DataFrame()\n",
    "        fs = 52 # frequence sampling is 52\n",
    "        f_cut = 1 # cutoff frequency\n",
    "        fs_n = f_cut*2.0/fs # normalized frequency\n",
    "        b,a = signal.butter(N=3, Wn=fs_n, btype='low')\n",
    "        for column in df.columns : \n",
    "            name = column+'_low-p'\n",
    "            df_r[name] = signal.lfilter(b,a,df[column].values)\n",
    "        return df_r\n",
    "    df_lp = lowpass(df, acc_nm)  \n",
    "    # High pass filter \n",
    "    def highpass(df, names):\n",
    "        \"\"\"Compute high-pass filter\"\"\"\n",
    "        df = df[names]\n",
    "        df_r = pd.DataFrame()\n",
    "        fs = 52 # frequence sampling is 52\n",
    "        f_cut = 1 # cutoff frequency\n",
    "        fs_n = f_cut*2.0/fs # normalized frequency\n",
    "        b,a = signal.butter(N=3, Wn=fs_n, btype='high')\n",
    "        for column in df.columns : \n",
    "            name = column+'_high-p'\n",
    "            df_r[name] = signal.lfilter(b,a,df[column].values)\n",
    "        return df_r\n",
    "    df_hp = highpass(df, acc_nm)\n",
    "\n",
    "    # Compute the total Total\n",
    "    df = pd.concat([df, df_med, df_diff, df_lp, df_hp], axis=1)\n",
    "    return df\n",
    "df_widen = widen_signal(df_raw)\n",
    "df_widen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def windowing(signal,size,step):\n",
    "    \"\"\"Compute the window\"\"\"\n",
    "    d = len(signal) #length of the signal\n",
    "    nk = int(np.floor((d-size+1)/step))+1 #le nombre de fenetres\n",
    "    wk = np.zeros((nk,size)) #windows\n",
    "    for j in range(nk):\n",
    "        wk[j,:] = signal[j*step:j*step+size]\n",
    "    return wk\n",
    "def window_labels(labels,size,step):\n",
    "    \"\"\"Compute the label of the window\"\"\"\n",
    "    d = len(labels) #length of the signal\n",
    "    nk = int(np.floor((d-size+1)/step))+1 #le nombre de fenetres\n",
    "    labelwk = np.zeros((nk)) #window labels\n",
    "    for j in range(nk):\n",
    "        labelwk[j] = np.max(np.argmax(np.bincount(labels[j*step:j*step+size])))\n",
    "    return labelwk\n",
    "def extract_windows(df,size,step):\n",
    "    \n",
    "    \"\"\"\n",
    "    extract windows with the specified size and step from the dataframe df\n",
    "    \n",
    "    Returns:\n",
    "    L : List of dataframes. Each dataframe contains a window extracted from each signal in df.\n",
    "    labels: labels of windows\n",
    "    \"\"\"\n",
    "    \n",
    "    L = []\n",
    "    n = df.shape[0]\n",
    "    L_windows = dict()\n",
    "    n_windows = int(np.floor((n-size+1)/step))+1\n",
    "    for column in df.columns:\n",
    "        if column not in ['label','label_str']:\n",
    "            L_windows[column] = windowing(df[column],size,step)\n",
    "    for i in range(n_windows):\n",
    "        ddf = pd.DataFrame()\n",
    "        for column in df.columns:\n",
    "            if column not in ['label','label_str']:\n",
    "                ddf[column] = L_windows[column][i,:]\n",
    "        L.append(ddf)\n",
    "    labels = window_labels(df['label'],size,step)\n",
    "    return L,labels\n",
    "\n",
    "\n",
    "\n",
    "def compute_features(df):\n",
    "    \"\"\"Compute features from a give dataframe\"\"\"\n",
    "    ## Basic Statistics\n",
    "    m = df.mean(axis=0).values # Mean\n",
    "    ma = df.mad(axis=0).values # Median\n",
    "    std = df.std(axis=0).values # Standard Deviation\n",
    "    var = df.var(axis=0).values # Variance\n",
    "    minimum = df.min(axis=0).values # Minimum\n",
    "    maximum = df.max(axis=0).values # Maximum\n",
    "    skew = df.skew(axis=0).values # Skewness\n",
    "    kurt = df.kurtosis(axis=0).values # Kurtosis\n",
    "    inteQ = (df.quantile(q=0.75, axis=0).values - df.quantile(q=0.25, axis=0).values) # Interquantile\n",
    "    r = np.hstack([m, ma, std, var, minimum, maximum, skew, kurt, inteQ]) # Compute vector of features\n",
    "    ## Auto-regressive coefficients\n",
    "    \n",
    "    ## Minmax\n",
    "    \n",
    "    ## Signal Integration\n",
    "    \n",
    "    return r\n",
    "\n",
    "\n",
    "def compute_matrix_data(df, N_samples=52, percentage=0.5):\n",
    "    \"\"\"Extract Matrix of data\"\"\"\n",
    "    df_X, df_Y = extract_windows(df,N_samples,int(percentage*N_samples))\n",
    "    X = compute_features(df_X[0])\n",
    "    for i in range(1,len(df_X)):\n",
    "        vec = compute_features(df_X[i])\n",
    "        X = np.vstack([X,vec])\n",
    "    \n",
    "    y = np.array(df_Y) # Compute the vector of labels\n",
    "    return X, y\n",
    "\n",
    "X, y = compute_matrix_data(df_widen) # Compute matrix of data\n",
    "# Put in a dataframe (More flexible)\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Methods\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Ensemble\n",
    "from sklearn.svm import SVC # SVM\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression # Logistic Regression\n",
    "## Utils\n",
    "from sklearn.model_selection import GridSearchCV # Choose parameters\n",
    "from sklearn.preprocessing import scale # Normalise matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0) # Shuffle data\n",
    "X_train = scale(X_train) # Scale the matrix of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lr = LogisticRegressionCV()\n",
    "# cv_lr = min(cross_val_score(lr, X_train, y_train, cv=5))\n",
    "# print(\"Logistic Regression :\"+str(cv_lr))\n",
    "# svm = SVC(C=8, kernel='rbf') # Parameters chosen by GridSearh\n",
    "# cv_svm = min(cross_val_score(svm, X_train, y_train, cv=5))\n",
    "# print(\"SVM :\"+str(cv_svm))\n",
    "# gb = GradientBoostingClassifier()\n",
    "# cv_gb = min(cross_val_score(gb, X_train, y_train, cv=5))\n",
    "# print(\"GradientBoosting :\"+str(cv_gb))\n",
    "# knn = KNeighborsClassifier() # Parameters chosen by GridSearh\n",
    "# cv_knn = min(cross_val_score(knn, X_train, y_train, cv=5))\n",
    "# print(\"Knn :\"+str(cv_knn))\n",
    "# lda = LinearDiscriminantAnalysis() # Parameters chosen by GridSearh\n",
    "# cv_lda = min(cross_val_score(lda, X_train, y_train, cv=5))\n",
    "# print(\"LDA :\"+str(cv_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Discriminant Analysis\n",
    "**  LDA **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lda(X,y, cla = None):\n",
    "    if cla==None:\n",
    "        cla = np.unique(y)\n",
    "        ix = np.in1d(y, cla)\n",
    "    else :\n",
    "        ix = np.in1d(y, cla)\n",
    "        y = y[ix]\n",
    "        X = X[ix]\n",
    "        \n",
    "    N_features = X.shape[1]\n",
    "    Sw = np.zeros((N_features, N_features)) # Within Matrix\n",
    "    Sb = np.zeros((N_features, N_features)) # Between Class Matrix\n",
    "    u = np.mean(X, axis=0)\n",
    "    ind = np.array([])\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        if cl in cla:  \n",
    "            index = np.where(y==cl)\n",
    "            Sw += np.cov(X[index].T)\n",
    "            Ni = len(index[0])\n",
    "            mn = np.mean(X[index], axis=0)\n",
    "            x = mn - u\n",
    "            x = x[:, None]\n",
    "            Sb += Ni*np.dot(x,x.T)\n",
    "    \n",
    "        \n",
    "    # Projection Matrix Theta\n",
    "    Proj_dim = np.unique(y).shape[0]-1\n",
    "    w,v =  np.linalg.eig(np.dot(np.linalg.inv(Sw),Sb))\n",
    "    Theta = np.real(v[:,0:Proj_dim])\n",
    "    ind = ind.astype(int)\n",
    "    projected = np.dot(Theta.T, X.T).T # Projected data\n",
    "    \n",
    "    return projected, ix\n",
    "\n",
    "# cla_t = [2,3,4,5]\n",
    "# Y, ix = lda(X,y, cla=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** KDA ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def w_matrix(x1,x2):\n",
    "    \"\"\" Matrix of i and j : 1/mk if i==j, 0 else\"\"\"\n",
    "    n = len(x1)\n",
    "    M = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        mk =  len(np.where(y==y[i])[0])\n",
    "        M[i,:] = np.equal(x1[i],x2).astype(int)*(1./mk)\n",
    "    return M\n",
    "\n",
    "def kda(X, y, cla = None):\n",
    "    if cla==None:\n",
    "        cla = np.unique(y)\n",
    "        ix = np.in1d(y, cla)\n",
    "    else :\n",
    "        ix = np.in1d(y, cla)\n",
    "        y = y[ix]\n",
    "        X = X[ix]\n",
    "        \n",
    "    K=rbf_kernel(X,X)\n",
    "    n = K.shape[0]\n",
    "    W = w_matrix(y,y)\n",
    "    # Within-class scatter matrix\n",
    "    Sw = np.dot(K,K)\n",
    "    # Between-class scatter matrix\n",
    "    Sb = np.dot(K,np.dot(W,K))\n",
    "    Proj_dim = np.unique(y).shape[0]-1\n",
    "    w,v =  np.linalg.eig(np.dot(np.linalg.inv(Sw),Sb))\n",
    "    Alpha = np.real(v[:,0:Proj_dim])\n",
    "    projected = np.dot(Alpha.T, K).T\n",
    "    \n",
    "    return projected, ix  \n",
    "\n",
    "# Y, ix = kda(X,y, cla=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def predict(projected, y):\n",
    "#     lr = LogisticRegressionCV()\n",
    "#     lr.fit(projected, y)\n",
    "#     y_pred = lr.predict(projected)\n",
    "#     return y_pred\n",
    "\n",
    "# C = confusion_matrix(y[ix], predict(Y,y[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from matplotlib.colors import Colormap\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(Y[:,0],Y[:,1], Y[:,2], c=y[ix], cmap='viridis_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "    \n",
    "# class_names = np.unique(y[ix])    \n",
    "# cnf_matrix = confusion_matrix(y[ix], predict(Y,y[ix]))\n",
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "#                       title='Confusion matrix, without normalization')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def predict(projected, y):\n",
    "#     lr = LogisticRegressionCV()\n",
    "#     lr.fit(projected, y)\n",
    "#     y_pred = lr.predict(projected)\n",
    "#     return y_pred\n",
    "\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from matplotlib.colors import Colormap\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(Y[:,0],Y[:,1], Y[:,2], c=y[ix], cmap='viridis_r')\n",
    "\n",
    "# confusion_matrix(y[ix], predict(Y,y[ix]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
